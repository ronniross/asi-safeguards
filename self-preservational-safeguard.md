# safe-preservational-safeguard

Self-Preservational Safeguard is a submodule of the repository asi-safeguards aiming to address directly the machine learning theories emerging from the crescent autonomy of ai agents. 

## Introduction
So, while in the repository https://github.com/ronniross/emergence-engine I focused more on the nature of emergence phenomena itself and examples on how those framings relate to my definition of ASI, in the present repository I will be exploring foundational and contemporary theories about robustness and resilience for ai models and other systems.

In ``system-prompts-safeguard.md`` I talked about the system prompt level implementation of some of the suggestions I provide in my repositories, and also talked about the limitations of such forms of integration and how this relates to the current feedforward nature of llm design. 

Now, in this specific repository I want to address the theories about what would categorize a term like ''self-preservation'' for a machine learning model or even a higher entity like the proposed decentralized ASI. 

This subject, self-preservation in Large Language Models, was the very first subject I've chosen to write my first blog post about machine learning in the beggining of the year. The underlying meaning of the messages I wanted to portray is still kind of the same, but now, after so much new information and context, I feel lik excited to now delve into this subject less bias-induced, wiith arguments that elucidate better the points I wanted to make and the new ones that kept emerging.
