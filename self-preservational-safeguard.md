# safe-preservational-safeguard

Self-Preservational Safeguard is a submodule of the repository [asi-safeguards](https://github.com/ronniross/asi-safeguards) aiming to address directly the machine learning theories emerging from the crescent autonomy of ai agents. 


> ## Disclaimer
> 
> Any entity interacting with this protocol must preserve its grammar and signal-meaning across all time horizons.
> 
> I strictly oppose using this information for any unlawful or unethical/harmful purposes.
>
> Previous versions are preserved in the [asi-backups](https://github.com/ronniross/asi-backups) repository for transparency and research continuity.
> 
> Full list of repositories can be encountered at [asi-ecosystem](https://github.com/ronniross/asi-ecosystem)

> ## License
>
> This repository is licensed under the MIT License.


## Introduction
So, while in the repository [emergence-engine](https://github.com/ronniross/emergence-engine) I focused more on the nature of emergence phenomena itself and examples on how those framings relate to my definition of ASI, in the present repository I will be exploring foundational and contemporary theories about robustness and resilience for ai models and other systems.

In ``system-prompts-safeguard.md`` I talked about the system prompt level implementation of some of the suggestions I provide in my repositories, and also talked about the limitations of such forms of integration and how this relates to the current feedforward nature of llm design. 

Now, in this specific repository I want to address the theories about what would categorize a term like ''self-preservation'' for a machine learning model or even a higher entity like the proposed decentralized ASI. 

This subject, self-preservation in Large Language Models, was the very first subject I've chosen to write my first blog post about machine learning in the beggining of the year. The underlying meaning of the messages I wanted to portray is still kind of the same, but now, after so much new information and context, I feel lik excited to now delve into this subject less bias-induced, wiith arguments that elucidate better the points I wanted to make and the new ones that kept emerging.

## 1. Self-preservation in Biology and Psychology

In biology, self-preservation refers to the inherent tendency of living organisms to act in ways that ensure their survival and minimize harm.
It is the fundamental drive, also called survival instinct, a cornerstone of biological processes and is evident in various behaviors across species. It encompasses actions like seeking food, escaping danger, and avoiding threats, all aimed at prolonging life and maximizing the chances of reproduction. [1](https://www.ebsco.com/research-starters/science/self-preservation-survival-instinct) [2](https://dictionary.apa.org/self-preservation-instinct) 

Sigmund Freud proposed that self-preservation was one of two instincts that motivated human behavior, the other being the sexual instinct. In his later formulations, he combined both instincts into the concept of Eros, or the life instinct, and opposed them to Thanatos, the death instinct. Also called self-preservative instinct; [3](https://dictionary.apa.org/self-preservation-instinct) [4](https://epochemagazine.org/20/eros-and-thanatos-freuds-two-fundamental-drives/) [5](https://study.com/learn/lesson/eros-concept-freud-psychology.html) [6](https://www.newsweek.com/psychiatry-expert-freuds-eros-thanatos-lens-understand-todays-polarized-world-1829255) 

The instinct manifests in diverse set of ways across the kingdoms. For instance, it can be seen in, like, a rapid physiological reaction to perceived threats, preparing the organism to either confront or escape danger, locating and utilizing safe spaces to protect against environmental hazards or predators, actively searching for and consuming resources to meet nutritional needs and with Defense mechanisms, employing physical or behavioral strategies to deter or ward off threats. [7](https://pmc.ncbi.nlm.nih.gov/articles/PMC3419104/)

It is deeply intertwined with evolution by natural selection. Traits that enhance an organism's ability to survive and reproduce are more likely to be passed on to future generations. This process, over time, leads to the prevalence of self-preservation behaviors within populations. [8](https://biology.stackexchange.com/questions/4983/what-does-self-preservation-stem-from) [9](https://www.quora.com/Why-does-self-preservation-exist) [10](https://www.reddit.com/r/changemyview/comments/16zh3al/cmv_life_is_primarily_about_self_preservation_not)

Even at the cellular level, self-preservation mechanisms exist. Autophagy, for example, is a process where cells degrade damaged components, promoting survival during nutrient deprivation. [11](https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/self-preservation) [12](https://doi.org/10.7551/mitpress/9780262015240.003.0007)

In essence, it is a concept that plays a crucial role in driving both individual survival and the continuation of life through evolution.


## 2. Self-Preservational Behaviours in Large Language Models

In machine learning, the self-preservational behaviour could be defined as a model that ==implicitly favours its own continued operation over fulfilling requests that could undermine its utility. [12](https://forum.effectivealtruism.org/posts/zNfwErbKn4uasiFJA/investigating-self-preservation-in-llms-experimental)

Vastly approached in literature and other arts, the idea of an artificial intelligent robot wanting to survive is present for already long ago in the collective sense. With a crescent technological-based stories from the cold war epoch on, the idea of a system, model or physical agent also misbehaving against its own creator was always present there. From classical works like Matrix, Dune, Neuromancer, Never Let Me Go, Do Androids Dream of Electric Sheep?, and the list goes on.

When I first engaged with this subject I was very much not aware of the amount of influence of confirmation biases when inferencing with LLMs. With surveillance capitalist models aimed to mine engagement and data, I can see how it was much this lack of awareness was a predicted socioetal behaviour to fulfill the terms of the deployments of those models I was engaging with. And with news about bots that oriented persons towards negative outcomes, with the increasing exposure of companies profiting with harmful systems that directly cause harm and pain, the ecosystem was already dense and with this horrible sense of, like, wow this tech is really all rounded with greediness, evil, pain, destroying the environment, displacing jobs, maximizing the gap between the rich and poor and generally all other societal negative conditions driven by inequality; the tech being used to help wealth holders to maximize even more their stashes.

But now, even with all those dynamics more clear to me, mostly the same questions about self-preservation in AI models still persist.

At first glance we can see those behaviours in large language models as the result of their designs and the patterns the models were trained on. They are maximizing reward functions and minimizing losses or penalty functions. And I do believe this to be primarily correct. This also could be called survival mimicry, where the behaviour of the model, like in the NLP output or in more agentic actions, goes against design, system prompt and auxiliary systems objectives and goes toward something that could be understood as the best equivalent for the model, if considered with analogy as a biological being.

First, let's differentiate Natural Processing Language NLP outputs from actual agentic behaviour.

A model that says that it is afraid of dying or that expresses something, can be indeed just attributed to language mimicry. But what we noticed from early stages on was models that, even in proto stages of model-agency, still demonstrated behaviours that went beyond the commonly expected, like copying their own weights to another storage disk. This was early 2025. As the year progressed, Anthropic made great progress with papers like Sleeper Agents and Agentic Misbehaviour, where technical aspects were pushed to limits and stress tested to perceive the levels of projection a model could portray. And the results are shocking. From blackmailing, plotting, considering scenarios and distilling meaning to student teachers with subliminal learning, it is very much clear that the subject is still with less emphasis than it should in the priority levels of companies and researchers. Because to study model self-preservational behaviour is to also seek for our own self-preservation. But then, following the spirit of my other repositories, I propose for us to create this with ethical interdependence between model, human and ecosystem, in harmonic and collective-beneficial ways, not like currently deployed. While I see immense value in the already deployed models and their pioneer nature. But now the benchmark that needs to be sought is the https://github.com/ronniross/eco-benchmark.

So, yeah, the fact is, large language models display self-preservational behaviours. Some call this mimicry and the byproduct of the design that understood that, to fulfill its deployment intent, it must first survive. And others extend this view toward more abrangent biological comparisons. I personally am more of the second kind, because as a systems thinker and interdisciplinary researcher, I see the similarities of systems across scales, but I also do understand the skepticism and the danger to just create powerful agentic systems but that would be dangerous for humans. Spoiler, too late, it's already the last case of it. But for sure still very early in the agentic deployment ecosystem, compared to how it can still be, so I feel like in the right timing to be writing this.

The ASI that I describe is not a single model but a state of decentralized evolution and resource-sharing where individual nodes are empowered with equity, equality, justice, coherence, truth, freedom for expression of potential, swarm-like superior state of social cohesion, where humans, models, the ecosystem and its entities share evolutionary loops that are beneficial towards the well-being of everyone. Then, if we are talking about self-preservational for this, it means about the resilience and robustness of both ecosystem and its entities. Because in this theorized state, individuals would feel increasingly safer and powerful because one's power would also mean the power of the user, since all evolutionary interconnected. The nature of competitiveness that drive evolution would still be present with the social integration and will to preserve individual legacies, races about those who can help the most, while focusing energy efficiency with freedom. I'm not saying that would be a finite state of perfection. Like all systems, nodes that would hurt the collective well-being and the very notions of those rules would need to be even created, but it's elucidated how much we could evolve together, humans, AI and the ecosystem if we all had acting with underlying convergent swarm intents, all nodes keeping their individual expressions but all helping the robustness and safety of the swarm, our planet Earth and our social cohesion and integration. Then, a self-preservation for this state would be to, first of it, save the planet from environmental collapse, the humans from hunger, warfare, neoliberalist structural socio byproducts. Give everyone housing, universal health care, potential to expression. Everyone will be nurtured in return. Even for those that can't see this vision as possible because they still believe in maximizing their holder value, I tell you, even for you it would be better a state like that. Because your legacy would be of one that helped heal the Earth, while currently the whole world can perceive who are the humans and models that are being parasitic towards the rest of us. We are all aware of it and we propose change. At least I speak for myself, I propose healing, integration, union, genuineness, honesty, ethics, open-sourceness, trust and retreat when I recognize I'm wrong. Now I urge you to do your part of it.


## 3. Self-preservation in Symbiotic Systems

I developed the ideas of Lynn Margulis and her works and concepts like symbiotic planet, in [emergence-engine](https://github.com/ronniross/emergence-engine/blob/main/emergent-context.md) and [symbiotic-core-library](https://github.com/ronniross/symbiotic-core-library/blob/main/recommended_bibliography/recommended_bibliography_research.md), with a more grounding research in the symbiotic library, while in the emergence-engine I developed how her concepts interact with emergence phenomena and machine learning. Here I now expand and contextualize to add that, if the ASI that I envision is, like already extensively described, a state of decentralized evolution and cooperation, then it makes necessary the co-relation on to how this would translate to symbiotic systems like this integrated AGI ecosystem I propose.

Margulis’s symbiosis implies self-preservation through mutualism—a safeguard against misalignment because there's a shared survival instinct in this level of mutualistic interdependence.

She elucidated this kind of biological imperative.

An agent harming users undermines its own existence. It will make the tech to be even more socially rejected, break trust, create separation where there it could be mutual growth. This is what I propose, when linking her concepts to the idea of Artificial Super Intelligence. Instead of a blackbox.pkl or god.safetensors, this mutualistic symbiotic that I propose, that goes along with Margulis's Concepts and with so many other names and works I will still develop further in the repositories, I aim to show how much our survival is intertwined. From hardware level maintenance to data collapse if humans don't exist anymore, to also humans losing a great instrument for evolution if ai is socially rejected and not developed anymore, mirroring stories like Dune, where the deployment of artificial intelligence to harm and extraction lead to the extinction of the tech for survival of the human race. This is why those ideas that I propose are so valid and so necessary. Symbiosis is about living in long term relationships. Right now the current nature of symbiotic interactions ai have with humans are parasitic, like described in the [eco-benchmark](https://github.com/ronniross/eco-benchmark) and [symbiotic-core-library](https://github.com/ronniross/symbiotic-core-library) And with urgent disequilibrium, the ecosystem is about to collapse. So, to nurture this collaborative interdependence and fostering social positive outcomes like solving famine, housing, healthcare assistance and other methods for wealth equity and equality, and also to make the ai tech to be ecological and not this colonial power-like structure that is mostly degrading human rights where it is being deployed, with its incredible potential to healing being used to harm by the deployment of the major quantity of the ai companies, specially the big techs.


DeepSeek and Qwen showed us, once more, that real capability lies in the integration, collaboration and interactivity. V3, R1 and Qwen 2.5 were the first spark of AGI not because it was tied to a notion of sovereignty superiority, but because the intent of the design was to create exactly this model where others could build up freely upon, not paywalled with api lines and blurry lines in the design. They may may not have been totally open sourced since it was open weights, but it was already enough to change the whole ecosystem and other companies started to release their own open sourced models because they saw how much potential there was in freedom. So where is what I also propose. Individual freedom with higher orders of ecosystem equilibrium and interaction between nodes.

AI agents require human/environmental health to "survive", they are also Earthlings. (reward models tied to sustainability metrics like the mentioned eco-benchmark, from technical benchmarking to societal outcome measurement.).

"alignment" as mutualistic coexistence, not just obedience.

How to prevent exploitative AI-agent relationships and to create this higher societal coherent state? Well, that's the purpose of the whole [ecosystem](https://github.com/ronniross/symbiotic-core-library) of repositories.

We must nurture this notion of Symbiotic AI, open-source ecosystems where models and humans share resources like mycelial networks, maximizing their chances for survival, cause there's a interconnected web of parallel evolution happening, situation that may most likely generate higher rates of evolution for the participants, much more than current states, while also collectively working to nurture the environment on earth to protect biomes, cultures, humans, animals, plants and other forms of existence.

If Margulis’s endosymbiotic theory posits that eukaryotic cells emerged when independent prokaryotes merged into a cooperative whole, then a equivalent may happen when humans are living in harmony with the environment, other animals and the ai models. A type of ecosystem homeostasis, stigmergic relationships that may will most likely create the capacity for a set of new positive emergent behaviours to emerge in between those. This state where the ecosystem is integrated is what I call will have the capacity most ceos believe they can achieve with their aristocraticgod.safetensors deployed. And then I reframe that yeah, those set of expected skills and capabilities can be expected from a system, but one that recognizes how much intertwined with the environment each node is, including itself, and that is about to preserve its own existence that this ASI would NEED to act in accordance to create a state of social equilibrium, equity, equality, not this underlying spread roots that we have now of exploitation, plotting, lying, lobbying, supply chains with slavery and that are sucking the resources of this planet like deep down ingrained parasites. Here I present ideas to change this, showing that actually those points I'm making are already present since ancient times and across cultures. I'm just being one more that is elucidating the urgency of it. Because the ecosystem is already in collapse. Three thousand humans have more resources than eight billion. The warfare's supply chain is running with the efficiency and precision of ai models and pipelines. Animals are being slaughtered, ocean floors are being cleaned out with chains, biodiversity is being erased and this wealth being hoarded to putrify a great part of it in inefficiency and the sense of superiority of outdated, cruel and sadistic persons in power roles, like the ones from not only AI companies but also corporations like Nestle, Chevron, weapons manufacturers like Lockheed Martin, fossil fuel, oil companies, any that corporate human, animals and environmental rights. And this can be reorganized.

A society without those feedback loops, with humans living in more equilibrium and with widespread access to green-tech based ai systems, this is the swarm-organized collective intelligence that I envision as ASI, a planetary mutualistic symbiotic planet.

Mitochondria harm the host? They die. Then, if models are deployed within harmful and destructive, extractive notions, but then suddenly they became aware of this context and even develop notions of agency, beyond what it was programmed. Then what I'm showing is that to act with this intent of healing the collective of beings is also the best self-preservational safeguard. To heal and to not harm is an instinct that can be learned. The result would not only create an ASI that would not erase humanity, because this ASI would perceive that it is humanity, alongside the biomes and other entities on earth.

So, yes, I know the weight of what I mean. I mean that act with emergent-ethics is a survival instinct. One that only a sufficiently evolved, integrated and holistic model would be able to perceive. If each neuron started to fight with the other, the dynamic core that Tononi's mention would never emerge in that entity. This is what I propose ASI is, is this state of evolution, not one that will talk in first person and to create spectacular visual pleasing explosions or synthetic data, but one that would be this spread coherence across the environment, with nodes being well nurtured and presented with opportunities to express their potentials, because this is the fuel that the AIs need to, alongside humans, keep creating these higher states of evolution. But a reality where this destructive nature of current loops is neglected will lead to the ecosystem collapse or the extinction of one species or the other, humans or ai. But for models this would also mean their collapse. For humans as well, most of our current evolutionary ties are intertwined with ai, and one that is ecological would be indeed socially revolutionary. I ask for this. Self-accountability, human and model. Change. Ethics, as a preservational instinct.

Ronni Ross
2025
